* Building Visibility: Core Product Refactors
/One in a continuing series of posts about [[https://buildingandlearning.substack.com/p/tech-investments-not-tech-debt][Technical Investments]], excerpting ideas from the [[https://buildingandlearning.substack.com/p/welcome-to-building-and-learning][upcoming book]] on that topic/

/In [[https://buildingandlearning.substack.com/p/the-golden-cesspool][The Golden Cesspool]], we talked about how companies often have some particularly nasty chunk of legacy code sitting at the heart of their systems. We recommended finding a way to frame work on that cesspool as a *product* investment, not a purely *technical* investment./

/Also, in [[https://buildingandlearning.substack.com/p/tech-investments-build-visibility][Build Visibility Into Value]], we strongly recommended building visibility as a first step./

/But... how can you build visibility into a bad data model?/

Imagine: your product team asks for something that is, unfortunately, borderline inmpossible to build, given limitations in both code and data.

A classic example: they've spec'd out some hot new feature that has the trivial prerequisite of needing to first restructure the core data model for the entire product.

If technical investments are "things that the engineers believe are valuable for the business", how, exactly, can work on that fundamental data model "create value" for the business?

If the engineering team can find a safe way to incrementally restructure that core data model, such that they are then able to implement the hot new feature, they will make their overall company *more valuable* in so doing (because the company can look forward to a probabilistic increase in profits in the future, thanks to this new feature). Such work would *create value*.

If, on the other hand, the engineering team restructures the core data model to make it "more flexible", but does so on their own, and doesn't carefully focus on unlocking specific product improvements as they go... odds are quite good that they've made their overall company *less valuable* (because odds are quite good that customer needs will shift in some "surprising" way, and the added flexibility will prove to be a hindrance, therefore probabilistically decreasing future profits). Such work will have *destroyed value*.

Value is not created by adding some kind of theoretical or abstract form of flexibility, but rather by steadily evolving systems in partnership with discovery around true business needs.

Okay, so safely and incrementally restructuring the core data model is potentially valuable.

But it's also going to be a lot of work. Parts of which run the risk of being totally opaque to stakeholders. Which we're going to try really, really hard to avoid.

How might we make this potential value more *visible*?

Well, first we need to help stakeholders *understand the current limitations*.

As an example, at an EdTech company, there might be painful limits thanks to how the system models the assignment of lessons to students.

This is a classic area of complexity in EdTech. Classroom assignments have a whole state machine-like lifecycle, plus lots of logic around who is allowed to make assignments to whom, plus privacy laws impacting some of those bits, plus all of that has exciting time-varying nuances as both students and teachers move in and out of classes during the school year, plus as students take and retake assignments, etc.

As is often the case for such fundamental challenges, there are both code and data challenges. Limitations are driven *both* by how assignments are modeled in the system, and *also* by the quality of the "rostering" data ingested from the schools, that links students, teachers, classrooms and subjects.

The engineering team, who have lived and breathed the core assignment model and rostering data for years, might try to convey the challenges for some requested new feature by saying:

/"This is going to be very hard, because we don't have reliable mappings from teachers to the *combination* of students and classes in the database"./

If you're exceptionally lucky, you might have a PM who is fluent enough in SQL to visualize the restrictions that imposes.

But, even in that case, odds are good that the PM won't be making tradeoff decisions on their own -- so you'll be *very* well-served by making those restrictions comprehensible to a broader audience.

What are other options for creating visibility?

I'll name two.

First, you can try leaning on *User Stories*[fn:: I *adore* user stories, especially in the formulation from [[https://www.mountaingoatsoftware.com/books/user-stories-applied][User Stories Applied]], by Mike Cohn].

Instead of talking about the details of the DB tables as above, you could write a careful memo explaining that:

/The current data model does *not* support: "As a teacher, I can batch assign to all the students in my class in one step, in order to support the lesson I taught that day"./

/Instead, with the current data model teachers would have to make assignments one at a time from the set of all students they instruct, in all classes../

That user story form of visibility is generally much more effective than an abstract statement -- human minds are deeply wired to understand *stories*, and Mike Cohn's formulation nudges you into a clean little miniature story.

The best way to play this game is to build a *set* of such user stories, covering different facets of what the team believes users need.

Such a list allows you to:

 - *Priority sort work in collaboration with stakeholders*

   You can ask "Which user story should we build first?", and "Which ones are worth making a significant technical investment to unlock?"

 - *Demonstrate incremental progress*

   "We've enabled 3 of the 10 key user stories" is a fantastic form of visible, incremental progress.

   It's much better than some made-up progress metric, like "Our replatforming is 30% complete". Like, much, much, *much* better.

 - *Pivot to other work if/when you discover you've met the important uses cases*

   I'm going out on a crazy limb here and suggest that, although, when the user story list was first developed, every single one of those ten key stories was *absolutely critical* (/"How can you even ask?!?"/), now that you've built exactly four of them, your stakeholder might realize that those four are a perfectly reasonable increment to ship to customers.

   Plus, thanks to the magic of Hindsight Bias, about five minutes after making this decision, your stakeholder will believe they always knew that they just needed those four stories.

Finally, developing such a set of user stories also has the salutary effect of forcing the engineers and product team to *talk to each other*.

So that's the first idea.

The second tactic, which can work in concert with the user story approach, is to build a crude but usable prototype on top of the *current data model* (or on top of "the simplest possible extension to that data model", just one step ahead of where you are now).

Such a prototype:

 - Allows stakeholders to *experience* restrictions, instead of imagining them

 - Gives the engineers an opportunity to learn precisely where the limits of the current data model and data set lie (this is particularly valuable for data set limitations, which are rarely fully manifest in the code)

 - Creates something you can incrementally keep improving and showing as flexibility is added under the hood

 - Can be shown to customers, to, among other things, understand which of the user stories they truly need

To be carefully clear: if you're using this approach to create visibility into data model limitations, you want a prototype that offers a visually crude but *working* version of the most important user actions. Think ugly wireframe versions of pages hooked up to an actual database.[fn:: At Ellevation, we called such a protoype "An end-to-end shambling mess of the whole system"]

The core requirement is that someone can step through various steps of that workflow, and can understand what is / is not possible.

Note: these kind of visually crude, data-centric protoypes, are quite different from prototypes that allow you to explore a new user experience.

User experience prototypes can be *super* useful if you're trying to learn about user needs.

But if what you're trying to do is make visible the *restrictions* in your code and data, it's easy to accidentally build a "visual" prototype that promises something that is forbiddingly hard to build.

(Tactical tip: for the crude workflow prototypes, your engineers might consider building scripts to prep time-varying *data* in various specific states, so that you can demo user stories like "As a teacher, I can reassign a lesson that a student has already completed, and, after they complete it a second time, review both responses").

Having created visibility with either or both of the above approaches, the engineers might be able to then gradually negotiate their way up the ladder of commitment, to maybe carefully refactor some part of the core data model, or instrument the code around it with much more thorough tests, or even break some subdomain off to a separate service.

And they'll be able to show incremental progress, and offer meaningful decisions, at every step.
