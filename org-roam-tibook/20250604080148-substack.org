:PROPERTIES:
:ID:       E0ADBF07-90B8-4C37-81C0-96A428020F5E
:END:
#+title: Substack
As part of my [[id:17305FA7-A43F-40C9-9309-0EF3577C70D0][Author Platform]], I've gone and set up a Substack.

* Tooling / Links

Substack Dashboard: https://buildingandlearning.substack.com/publish/home

* Substack Buttong HTML
<iframe src="https://buildingandlearning.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>

* Ideas for posting cadence
3x / week.

Pick out 2 things I've written, and 1 reference to something I like.

Schedule those for, like, Mon/Wed/Fri.

Do it... when? Either EOW, when I'm done writing?

Do it in the afternoon, maybe on Mondays or some such.

For now, reference or all of them on LinkedIn?

** Email to EWJ re posting cadence
Subject: Rubber Ducking Substack Posting Cadence
E-

I want to write out some thoughts about posting cadence plus patterns for Substack, so I can see if I can clarify my own thinking (and of course I super value your take on the below).

So, to start, what is the value of, the goals for posting to Substack:

 - Develop a pool of beta readers

 - Build a seed audience for when the book launches

 - Build a long-term platform

 - Create motivation and connection (by e.g. hearing that what I'm writing is useful)

I think that, if I can make the substack itself recommendable, it helps on all those fronts.

And, even in starting to work on it, I'm discovering something which is kind of blindingly obvious in retrospect, which is:

Because I'm already spending hours and hours every day *writing a book*...

...I have *tons* of content available to post.

And thematically related content, etc.

And I'm not gonna, like, run out, any time soon.

So then the question kind of becomes: how often should I post? Should I worry about annoying people?

I have 3 posts scheduled for this week -- each about 500 words, each, I think, pretty fun and tight (and each having, like, one insight that I think people might say "Oooh, I want to tell someone this").

I took me very little time to get those set up (most of the time was just figuring out how to deal with formatting and scheduling on substack). Cutting and pasting out of all this writing I'm doing is not hard.

So, one option is to post every (week)day. And celebrate that fact.

I could do something like:

 - Mon/Wed/Fri are ~500 word excerpts from the book

 - Tue is Tuesday Thank You Notes, where I spend a couple hundred words celebrating some book or blog post or paper I've found influential in my thinking. I've got a list, this would be fun and easy to write, and create some variety.

 - Fri is Questions for the Community -- something where I'm asking people to send in stories or questions or the like, on some theme (and later sharing back what I hear).

I mean, having written that out, why... wouldn't I do it?

I think there's decent evidence my writing isn't annoying, and it's email, they can just ignore it if they want.

It sounds like a thing that people might well start recommending, esp if it's at all playful and/or fun. Plus if I can get a Friday Mailbag going, I think I would find that super motivating (plus, frankly, great early customer development for later Highly Paid Consulting gigs, if I want to go down that road)

I can likely get the work down to ~1-1.5 hours a week, to prep 5 days worth of posts, out of the stream of writing I'm alrady doing.

Could just build a hbabit around that for, like, Wednesday afternoons, to prep next week's posts.

And any fears that "people won't need to buy the book if it's all out there on a blog" are just batshit crazy and dumb. If the blog gets enough traction that I have *any* worries on that front, I'm selling a metric ton of books, *because* of the traction, not despite it. Just need to like beat myself up over that fear or something.

Thoughts?
D
* About

*Building & Learning*

aka

Some Adventures At the Intersection of Engineering, Product & Human Nature Whilst Engaged in the Pursuit of Value

aka

Some Things I've Learned Both From & With All The Excellent People With Whom I Have Worked Over These Years

aka

Build -> Learn -> Build -> Learn

aka

The Name of the Song is Haddock's Eyes, and The Tune's My Own Invention

(No, *you're* obsessed with Lewis Carrol, shut up)

Hi, my name is Dan.

For the last 30 years, I've worked in various combinations of engineering and product -- I love both discplines, and value both super highly.

I've worked as a principal engineer building and operating complex systems; I've led product at startups desperately trying to find traction; I've managed engineering teams ranging in size from 2 to 500.

Right now, I'm working on a book:

"Tech Investments, Not Tech Debt: How To Make Time For Engineering Work That Matters".

In this herein Substack, I'll be sharing ideas from it and thoughts the writing of it triggers and the like.

(and just possibly some random Lewis Carrol-inspired absurdity once in a while, I make no promises otherwise)

Speaking of said book, if you're potentially interested in being a beta reader at some stage of development, holler at me.


* Posts
** Defining Tech Investments

I believe that the metaphor of "Technical Debt" started it's life as a very useful prompt for economic thinking...

...but has now mostly drifted into a not-very-useful shorthand for *bad code*.

For reasons that apparently require a book to fully explain, I don't think engineering leaders or teams should spend much time talking about "bad code".

Instead, I recommend spending time identifying and advocating for *"Technical Investments"*, which I define as:

Work that the engineers believe is *valuable to their business*...

...but that *no one is asking for*.

The Technical Investment approach, when applied well, leads to very-useful-indeed conversations around:

 - Potential business value

 - How to make that value visible to key decision makers

 - How to safely and incrementally realize that value

Of course, bringing stakeholders along on that journey is somewhat profoundly non-trivial, so I'll be spending real time in the book on how to build trust and rapport with stakeholders, as an enabler for making major technical investments.

** Tech Investments - Build Visibility Into Value

Instead of technical debt, engineering leaders can ask their teams *and* stakeholders to talk about *technical investments*, which I'll define as:

/Work the engineers believe is *valuable for the business*, but that *no one is asking for*./

That puts the focus on the genuine problem: a *mismatch in understanding* between the engineers and their stakeholders, about what is *potentially valuable* for the business.

At heart, the vast majority of both engineers and stakeholders want to create value for the business.

They just have different information and beliefs about how best to do so.

Many engineers try to resolve this gap by *explaining* the potential value: "You see, when code has bad 'coupling', a change in one place can impact many other places, which is a drag on development. This is why we should spend a week refactoring."

Although there's a good instinct in this -- bringing the stakeholders into a shared understanding with the engineers about what is valuable -- it has one crucial flaw:

It requires the stakeholders to take the entire statement of value on faith.

There is nothing they can *see*, that shows them things are "bad", before the investment is made.

And there will be nothing they can *see*, after, that shows them things have gotten "better".

Given that lack of visibility, it's hardly surprising that stakeholders, confronted with such a choice, often feel like they are giving something up and getting nothing in return.

One of the core theses of the book is that engineering leaders have a wide variety of options to *build visibility* into potential value.

It is *massively cheaper* to build such visibility than it is to make the full investment.

Once there is visibility, the engineers and their stakeholders can look at it, together, and operate from a shared understanding of the reality of the business.

If the engineering team can then offer disciplined, incremental steps to gradually (and visibly) improve things (including improving the depth and reach of the visibility), theu can build real trust with stakeholders over time.

That then allows them to "climb the ladder" from small initial investments to, sometimes, very major, transformative investments.

The best way to do that is not as a one-off project an engineering leader puts all their authority on the line for (ugh, I've done that so many times, and never seen great results), but instead, a series of repeated *tech investment cycles*, each of which generates visibility and options.

All in partnership, not opposition, with stakeholders.
** Tech Investments: Favor Repeated Cycles Over One-Off Projects

If you're stuck in an oppositional relationship with stakeholders who don't seem to care about anything except their feature list, it may feel like your best bet is to carve off a big block of time so the team can just go and definitively fix their problems, without interruptions.

I have tried the "Bargain for one giant chunk of time" approach, and, unfortunately, it has been something of a consistent disaster.

Technical investments are *far* more effective done as a series of repeated small steps, instead of a single giant one-off project.

There are at least two reasons for this.

First, in the big bang mode, the stakeholders don't tend to see the work as "valuable".

Instead they see it as a "painful delay", and as such, feel "owed" immediately faster progress on their features.

Which is not always the immediate payoff for a technical investment -- even an extremely valuable one.

# XXX Add: especially if the big bang investment has no associated visibility?
# XXX Tease apart: don't start here vs it's okay to build to this
# As in the real problem is if you use the one-time nature to avoid fully educating the stakeholder and ensuring they can see the results of the investment.

But the big bang approach is not actually good for the engineers, either.

Real value is often created at the intersection of the technical and human/social systems (see Allspaw on Socio-Technical) -- and those are essentially impossible to adjust in big, fixed steps.

E.g. two significant forms of value are:

 1) reducing the time to get code to production

 2) reducing the time to restore from outages

# XXX increasing the load a system can handle? Increasing capacity to match current demand?

Both are *extremely* valuable for a business (more on both in [[id:E7DB3CD4-9B7B-425B-BF07-E2607DDD6670][Forms of Value/Visibility]]).

But neither is effectively addressed as a single big bang investment -- you have to steadily improve things, see where new bottlenecks or problems occur, and then pick the next thing to focus on. That kind of effort take real calendar time -- you have to see a set of "improved" deploys, or see how the team is able to handle the next set of stresses to the system, before you can understand your next step.

# Footnote?
(if an engineer tries to convince you that all the stability problems in the site will be addressed by rewriting the entire thing in Rust, you should +fire+ firmly persuade that engineer to think otherwise).

# Maybe: do a single one in detail, then list a slew of others which also need steady digging and learning

Thus, what you want is to get into a *cycle of technical investments* -- where you are repeatedly identifying small potential improvements, advocating for those, and then executing on them.

Going through this cycle *with* your stakeholders will gradually build trust and rapport over time.

# (and a shared understanding)

That increased trust, rapport and understanding will allow you to "lever up" to larger and larger investments.

You should, of course, still *execute* the work in incremental steps (because that is how all software should be built) but you can use these repeated cycles to gradually climb the [[id:722C702D-A6C2-4A51-AB62-515CE8144AA2][Ladder of Commitment]] for technical investments:

 - On the Side

 - A Single Ticket

 - A Within-Team Project

 - A Cross-Team Initiative

 - A Durable Team
** The Golden Cesspool

Your product team has an ambitious plan to build something cool.

Good news: customers are genuinely eager to use this cool thing!

Bad news: building it will require your team to dig into that horrible part of the legacy codebase that was written by contractors many years ago, has no tests that you can trust worth a damn, and is based on a data model that is a malevolent joke against your current reality.

I will make a bold prediction: this specific flavor of technical debt is never going to go out of style.

The inimitable Edmund Jorgensen (who I quote, um, kind of a lot in the book), first framed this most clearly to me, and named this pattern:

[[https://www.tomheon.com/2017/03/24/the-golden-cesspool/][The Golden Cesspool]]

/"Proposing the rule of the golden cesspool: the closer code is to the heart of a businessâ€™s domain, the worse the code will be."/

In my experience, Edmund is totally right.

Almost all companies seem to have some genuinely hideous mass of code sitting at the beating heart of their business.

Over the years, tons of complex business logic has been shoved into that cesspool.

The most critical processes of the business are tied to data updates in the cesspool.

All sorts of state gets updated in all sorts of deeply non-obvious ways.

Your engineering team likely already thinks about this morass as a prime example of tech debt, and are itching to rewrite it.

They may, in fact, resist an attempt to methodically build visibility and then incrementally improve things.

Instead, they'll make the faux-economic argument that it'd be better to just commit to a ground-up rewrite of that core system.

They'll claim that it will be both faster and cheaper to do so "from scratch" rather than through a slow, incremental shift.

Once that rewrite is fully finished, the things the product team are asking for will be super easy to build!

What could go wrong?

Please see our later chapter: *The Giant Rewrite: Only Undertake If You Wish To Later Be Fired Midway Through a Long & Painful Death March*, which has some tips on how one *can* gradually rewrite such systems.

But as a brief teaser: the core trick here is to convert this from a technical investment to a *product* investment.

The product team *already thinks* there's value here -- they just don't realize there's more of a *cost* than usual.

Making those risks visible and gradually wearing them down will therefore create value.
** Building Visibility: Core Product Refactors
/One in a continuing series of posts about [[https://buildingandlearning.substack.com/p/tech-investments-not-tech-debt][Technical Investments]], excerpting ideas from the [[https://buildingandlearning.substack.com/p/welcome-to-building-and-learning][upcoming book]] on that topic/

/In [[https://buildingandlearning.substack.com/p/the-golden-cesspool][The Golden Cesspool]], we talked about how companies often have some particularly nasty chunk of legacy code sitting at the heart of their systems. We recommended finding a way to frame work on that cesspool as a *product* investment, not a purely *technical* investment./

/Also, in [[https://buildingandlearning.substack.com/p/tech-investments-build-visibility][Build Visibility Into Value]], we strongly recommended building visibility as a first step./

/But... how can you build visibility into a bad data model?/

Imagine: your product team asks for something that is, unfortunately, borderline inmpossible to build, given limitations in both code and data.

A classic example: they've spec'd out some hot new feature that has the trivial prerequisite of needing to first restructure the core data model for the entire product.

If technical investments are "things that the engineers believe are valuable for the business", how, exactly, can work on that fundamental data model "create value" for the business?

If the engineering team can find a safe way to incrementally restructure that core data model, such that they are then able to implement the hot new feature, they will make their overall company *more valuable* in so doing (because the company can look forward to a probabilistic increase in profits in the future, thanks to this new feature). Such work would *create value*.

If, on the other hand, the engineering team restructures the core data model to make it "more flexible", but does so on their own, and doesn't carefully focus on unlocking specific product improvements as they go... odds are quite good that they've made their overall company *less valuable* (because odds are quite good that customer needs will shift in some "surprising" way, and the added flexibility will prove to be a hindrance, therefore probabilistically decreasing future profits). Such work will have *destroyed value*.

Value is not created by adding some kind of theoretical or abstract form of flexibility, but rather by steadily evolving systems in partnership with discovery around true business needs.

Okay, so safely and incrementally restructuring the core data model is potentially valuable.

But it's also going to be a lot of work. Parts of which run the risk of being totally opaque to stakeholders. Which we're going to try really, really hard to avoid.

How might we make this potential value more *visible*?

Well, first we need to help stakeholders *understand the current limitations*.

As an example, at an EdTech company, there might be painful limits thanks to how the system models the assignment of lessons to students.

This is a classic area of complexity in EdTech. Classroom assignments have a whole state machine-like lifecycle, plus lots of logic around who is allowed to make assignments to whom, plus privacy laws impacting some of those bits, plus all of that has exciting time-varying nuances as both students and teachers move in and out of classes during the school year, plus as students take and retake assignments, etc.

As is often the case for such fundamental challenges, there are both code and data challenges. Limitations are driven *both* by how assignments are modeled in the system, and *also* by the quality of the "rostering" data ingested from the schools, that links students, teachers, classrooms and subjects.

The engineering team, who have lived and breathed the core assignment model and rostering data for years, might try to convey the challenges for some requested new feature by saying:

/"This is going to be very hard, because we don't have reliable mappings from teachers to the *combination* of students and classes in the database"./

If you're exceptionally lucky, you might have a PM who is fluent enough in SQL to visualize the restrictions that imposes.

But, even in that case, odds are good that the PM won't be making tradeoff decisions on their own -- so you'll be *very* well-served by making those restrictions comprehensible to a broader audience.

What are other options for creating visibility?

I'll name two.

First, you can try leaning on *User Stories*[fn:: I *adore* user stories, especially in the formulation from [[https://www.mountaingoatsoftware.com/books/user-stories-applied][User Stories Applied]], by Mike Cohn].

Instead of talking about the details of the DB tables as above, you could write a careful memo explaining that:

/The current data model does *not* support: "As a teacher, I can batch assign to all the students in my class in one step, in order to support the lesson I taught that day"./

/Instead, with the current data model teachers would have to make assignments one at a time from the set of all students they instruct, in all classes../

That user story form of visibility is generally much more effective than an abstract statement -- human minds are deeply wired to understand *stories*, and Mike Cohn's formulation nudges you into a clean little miniature story.

The best way to play this game is to build a *set* of such user stories, covering different facets of what the team believes users need.

Such a list allows you to:

 - *Priority sort work in collaboration with stakeholders*

   You can ask "Which user story should we build first?", and "Which ones are worth making a significant technical investment to unlock?"

 - *Demonstrate incremental progress*

   "We've enabled 3 of the 10 key user stories" is a fantastic form of visible, incremental progress.

   It's much better than some made-up progress metric, like "Our replatforming is 30% complete". Like, much, much, *much* better.

 - *Pivot to other work if/when you discover you've met the important uses cases*

   I'm going out on a crazy limb here and suggest that, although, when the user story list was first developed, every single one of those ten key stories was *absolutely critical* (/"How can you even ask?!?"/), now that you've built exactly four of them, your stakeholder might realize that those four are a perfectly reasonable increment to ship to customers.

   Plus, thanks to the magic of Hindsight Bias, about five minutes after making this decision, your stakeholder will believe they always knew that they just needed those four stories.

Finally, developing such a set of user stories also has the salutary effect of forcing the engineers and product team to *talk to each other*.

So that's the first idea.

The second tactic, which can work in concert with the user story approach, is to build a crude but usable prototype on top of the *current data model* (or on top of "the simplest possible extension to that data model", just one step ahead of where you are now).

Such a prototype:

 - Allows stakeholders to *experience* restrictions, instead of imagining them

 - Gives the engineers an opportunity to learn precisely where the limits of the current data model and data set lie (this is particularly valuable for data set limitations, which are rarely fully manifest in the code)

 - Creates something you can incrementally keep improving and showing as flexibility is added under the hood

 - Can be shown to customers, to, among other things, understand which of the user stories they truly need

To be carefully clear: if you're using this approach to create visibility into data model limitations, you want a prototype that offers a visually crude but *working* version of the most important user actions. Think ugly wireframe versions of pages hooked up to an actual database.[fn:: At Ellevation, we called such a protoype "An end-to-end shambling mess of the whole system"]

The core requirement is that someone can step through various steps of that workflow, and can understand what is / is not possible.

Note: these kind of visually crude, data-centric protoypes, are quite different from prototypes that allow you to explore a new user experience.

User experience prototypes can be *super* useful if you're trying to learn about user needs.

But if what you're trying to do is make visible the *restrictions* in your code and data, it's easy to accidentally build a "visual" prototype that promises something that is forbiddingly hard to build.

(Tactical tip: for the crude workflow prototypes, your engineers might consider building scripts to prep time-varying *data* in various specific states, so that you can demo user stories like "As a teacher, I can reassign a lesson that a student has already completed, and, after they complete it a second time, review both responses").

Having created visibility with either or both of the above approaches, the engineers might be able to then gradually negotiate their way up the ladder of commitment, to maybe carefully refactor some part of the core data model, or instrument the code around it with much more thorough tests, or even break some subdomain off to a separate service.

And they'll be able to show incremental progress, and offer meaningful decisions, at every step.
** The Tech Investment Cycle

I believe that software engineers should stop talking about Technical Debt.

I believe they should instead, advocate for *Technical Investments*, which are defined as:

/Work that the engineers believe is *valuable to their business*.../

/...but that *no one is asking for*./

(see: [[https://buildingandlearning.substack.com/p/tech-investments-not-tech-debt][Tech Investments, Not Tech Debt]])

As per [[https://buildingandlearning.substack.com/p/tech-investments-favor-repeated-cycles][Favor Repeated Cycles Over One-Off Projects]], the best way I've seen engineers use this concept is by way of an ongoing, iterative series of increments of work -- not by way of giant, big bang investments.

Within each of those "cycles", engineer leaders will go through an series of steps, a sort of fundamental framework for leading a technical investment:

 - Identify issues the engineers are *worried about*

 - Map those issues into *potential value* for the business

 - *Build visibility* into that potential value

 - Develop *options* for small increments of investment

 - Share options and visibility with *stakeholders* to obtain a commitment of time

 - Do <Something> In That Time

 - Celebrate visible improvements via *story-telling*

 - Start a new cycle, with more *visibility and trust*

Not every single cycle needs to go through every step -- but it's good to understand this as an overall *arc* for your team and your stakeholders.

If you find yourself stuck, you can check if you've tried to skip past something important.

E.g. "Oh, our conversations with the stakeholder feel stuck because we have no visibility to offer".

Or, "We need to come up with some incremental options before we propose an investment".

Or, "Before we start over, who have we told about this win?"

Let's bring the cycle to life with a story.[fn:: This is a "100% true as I remember it" story. If any participants remember any details differently, please let me know!]

*** The Horror of Legacy App Deploys

Partway through 2020, I joined Ellevation Education, a thriving EdTech company serving the educators who work with English Learner students at public schools across the country.

As I ramped up, I discovered that the engineers had grown incredibly frustrated with deploying the legacy app.

Not all development ran through that app... but when it did, it *sucked*.

The legacy app deploy pipeline featured:

 - A hodgepodge of jobs spread across multiple automation platforms (Jenkins, Octopus, some GitHub bits)

 - Several key steps that needed to be manually kicked off once previous steps completed

 - Cryptic job failures that only the most senior engineers could resolve

 - A sprawling suite of poorly-maintained browser-driving front-end tests that enjoyed the properties of being both slow *and* flaky

The #deploy-sucks Slack channel was just a firestorm of angry gifs and emojis.

But... what was the product team's experience?

First, please note: Ellevation had a pretty technical savvy product team -- Nathan Papazian, Ellevation's VP of Product[fn:: and now CPO!], very much tried to hold his PM's accountable for listening carefully to the engineers, and understanding the tradeoffs in their systems.

But what could that product team *observe*?

Well, any development that touched the legacy app felt slow.

But, legacy app development *always* felt slow.

And there were plenty of other contributing factors -- understanding of the legacy app was poorly distributed throughout the team (which of course was made worse because deploying it was a nightmare, so everyone avoided it like the plague).

Also, the engineers were complaining about legacy app deploys.

But, to a first approximation, engineers are *always* complaining about deploys. So this didn't really stand out.

Furthermore, when the product team asked the engineers for any concrete improvement options, the engineers weren't able to offer much in the way of specifics -- the whole thing was such a mess, it wasn't clear where to start.

One engineer kept saying "We need to rewrite all our front-end Robot tests", but that was clearly an apocalyptic amount of work.

And so they all felt stuck.

Then, one afternoon, while waiting for a deploy to finish, Alla Hoffman, a very bright and very frustrated engineer threw together a spreadsheet and asked all the engineers on the team to just *manually* log their deploy times in it (the eng team called the legacy app Flagship, so Alla titled the spreadsheet "Flagship Misery").

She asked engineers to fill in their name, one column when they started the first in the series of jobs, then another when the final job finished up. There was also a column for free text notes on anything that happened.

Setting up that spreadsheet took her about 30 minutes (counting the, ahem, vigorous email she sent to all of engineering encouraging them to keep it up to date).

# This was a technical investment!

In so doing, Alla *created visibility* -- which is an *excellent* form of value for a business, because it allows people to *make better decisions*.

She did so as an "on-the-side" project -- one where the engineers *don't* ask the product team for permission/capacity, but just quietly scrape together a bit of time.

The major thesis of the book is that engineers can and should develop a collaborative partnership with stakeholders around technical investments.

But some work is still best done without a formal negotation. That's an especially good pattern for cheap initial steps to build visibility.

I'll talk in a chapter titled "The Ladder of Commitment" about different scopes for technical investments, and where the on-the-side approach works, and were it falls down.

Once Alla had set up that spreadsheet, what happened next?

The engineers on the team were plenty motivated to track their deploys (and had plenty of time to do so, thanks to the various forms of failure). They didn't experience this as annoying manual overhead -- rather, they experienced it as validation for their pains, and a chance to contribute to a better future.

After a few short weeks, Lisa McCusker, Ellevation's engineering manager over that domain, brought the spreadsheet to Nathan and the product team.

Together, they all looked at how long it was taking to get legacy app changes out to production -- and discovered that, on occasion, there were so many repeated failures, it took *more than a full day* to get a single deploy out. The comments were filled with complaints about flaky tests and mysteriously stuck jobs.

At this point, it wasn't hard for Lisa to convince Nathan to carve out a week for Alla to go back and instrument the key stages of the deploy process, so they could better understand what the hell was going on (this is what I call "Ticket" scope).

Thus, a few weeks later, they were looking together at a clearer picture of overall deploy trends and, for various internal stages, both times and failure rates.

The flaky WebRobot front-end tests proved to be the worst culprit -- often needing to be re-run multiple times until they passed.

But, unfortunately, there was no simple fix -- it was tempting to just rm -rf the whole set, but everyone agreed that, on occasion, the tests would catch a *very* bad problem in some ancient part of the legacy product that customers still depended on.

Lisa made a case for a carefully time-boxed, three week-long effort by a couple of engineers, to inventory all the tests, come up with options, and share those back with her and Nathan (this is "Project" scope).

Note that what Lisa offered had a built-in *off-ramp*, partway through: when the engineers shared options, she and Nathan could decide to pause the rest of the project.

With the potential value, the increment and the off-ramp all clear, Nathan was ready to commit.

He and Lisa worked together to find a time for this project -- they weren't working much in the legacy app at the moment, but both knew a big chunk of work on it was coming, and they were *both* motivated to get deploy improvements in before it landed.

With some careful co-planning, they found a chunk of capacity.

When the engineers dug in, the product team worked closely with them. Product and engineering decided together which features were most important to retain test coverage for, and which areas were okay to leave with less coverage.

Thanks to having built shared understanding, the product team were ready to pitch in and do this work together.

When, after a few weeks, the engineers brought options to Nathan and Lisa, they all decided together to have the engineers do two things.

First, they just flat out deleted a big set of tests (deleting code is Lisa's absolute favorite thing to do, she was very happy on that day).

Second, they moved the remaining flaky-but-sometimes-valuable tests off the main deploy path -- they only ran that full suite for the small subset of deploys that touched certain parts of the legacy app. The rest only ran the core tests, which did *not* exhibit random transient failures.

Those two steps immediately made the vast majority of legacy deploys much faster.

Also, Nathan, Lisa and the entire team could all *see* that improvement on the graphs of average deploy time.

Because, as a small, ticket-sized follow up, Alla had piped the deploy times into Grafana so the team and the PM's could visualize them over time.

For a few more months the team kept steadily chipping away at the deploy process, in parallel with a great deal of feature work.

Sometimes it was just a ticket here or there, sometimes an engineer would drop off the main sprint for a week or even a month and just focus on some specific challenge.

Eventually, the legacy app deploys became "good enough", and, by common agreement between Lisa and Nathan, the pace of investment in this specific area slowed.

To be clear, legacy app deploys were still far from ideal!

But they were enough better, that further investment didn't seem indicated at that point.

Then, one day, the legacy app suffered a major outage.

In the course of resolving the incident, the team rapidly deployed one change after another, first to diagnose and then to fix the underlying issue.

When Lisa wrote up the post-mortem notes, she took time to carefully document how the fast, reliable deploys had saved Ellevation somewhere between one and three *full days* of downtime.

She made a point of sharing those post-mortem notes with both the product team and the CEO -- a fundamental enabling strategy for technical investments is to "Make Your Post-Mortems an Act of Visibility".

All of which eventually led to Ellevation's (highly non-technical!) CEO, Jordan Meranus, beaming with pride at a company All Hands as Lisa and Alla told *the entire company* the story of how the team had gradually improved deploys.

The human mind is deeply wired to remember *stories*, so it's well worth your time to use storytelling structures to make technical investment wins feel vivid, real and meaningful to a broad audience.

In later posts, I'll be digging into the stages of the technical investment cycle, and sharing what I have learned about running them well.

Finally, if you're reading this at some point in the summer of 2025, I'll make a request:

Could I ask you to let me know what you're most eager to learn more about, with regard to tech investments / tech debt?

That could be something I've touched on, or possibly something I have *not* touched on, but which seems important for running this game plan. I can be reached through substack, or via milstein.dan@gmail.com

Also, of course, share this with anyone you think would find it useful!

** Engineering & The Creation of Value, Part I
I believe that software engineers should stop talking about Technical Debt.

I believe they should instead, advocate for *Technical Investments*, which are defined as:

/Work that the engineers believe is *valuable to their business*.../

/...but that *no one is asking for*./

(see: [[https://buildingandlearning.substack.com/p/tech-investments-not-tech-debt][Tech Investments, Not Tech Debt]]).

And an absolutely central question is:

*"What is valuable to a business?"*

Let's talk.

*** Warning (Extremely Valuable) Abstract Thinking Ahead

We're about to do an unapologetically deep dive into how, exactly, engineers create value for the businesses in which they operate.

This may feel somewhat far removed from the day-to-day concerns that are filling up your inbox and/or Slack.

Nonetheless, I'll encourage you to take the time to dig in and really wrestle with the below.

Precisely *because* few people really understand it, having that insight into the creation of value is like having a superpower.

Such understanding can, over time, meaningfully help your career -- because it will allow you to find creative ways (that no one is asking for!) to generate major, visible business wins.

It can also make your job profoundly more pleasant. Because you can break out of a lot of frustratingly oppositional patterns, by finding ways to pursue value that work for everyone.

But developing such an understanding takes some real work. [fn:: My friend Edmund says "Tell them sometimes you need the math so fucking clear an afternoon and brew some coffee".]

*** What Is Value?

Technical Investments are defined as:

"Work that engineers believe is *valuable for the business*, but which no one is asking for".

In order to identify and advocate for technical investments, engineering leaders must understand *how engineers create value for a business*.

The first question is:

*What, precisely, do we mean by 'value for a business'?*

I suspect most engineers, if they think about business value at all, tend to think only of *revenue* -- e.g. if their team is working on a new product, they'll think of "business value" as "the dollars that customers pay us for that product, once it launches".

This is far too limited a view.

Revenue, *by itself*, is not a reliable proxy for overall company value -- revenue plays a *role* in determining company value, but is not that value by itself.

So, what is "overall company value"? And how might we determine it?

Interestingly, there are two situations where we can *easily* obtain an estimate of overall company value:

 - Immediately after someone buys or sells a share of stock in a public company

 - Immediately after a funding round at a startup

In both cases, investors are putting up *their own money* to buy a *part* of a company.

How much those investors are willing to pay for that *part* of the company is driven by their belief about the overall company value.

That is the kind of value we're interested in. [fn:: I will bet you All the Things that the leader of your business cares very intensely about this form of value. Like, very, *very* intensely.]

Engineers can, by the work they do, either increase *or decrease* that kind of overall company value.

The potential increase in value is why companies hire engineers -- engineer salaries represent an *investment* on the part of the business, made in hopes of a positive return.

Whatever someone tells you your job is, please understand and believe me: your *actual* job is to increase the value of your company.

Unfortunately, company executives tend to have a somewhat limited understanding of how the work of engineers can increase the value of the company.

This is especially true for various value opportunities that engineers can see right in front of them, e.g:

 - Cleaning up difficult-to-change code

 - Improving tooling to test, integrate and deploy changes

 - Instrumenting production systems with monitoring

 - Retiring old infrastruture

In the right situations, the above kinds of work can make a company meaningfully more valuable -- and can thus be worth prioritizing, even against work that might immediately increase revenue.

But engineers can not, in general, depend on stakeholders or product managers to understand how to convert problems in technical areas into investment opportunities.

That is why "no one is asking for" the work the engineers believe is valuable -- because they don't understand how it can ultimately make the company worth more.

To ensure that you, as an engineering leader, are able to do that work, we're going to spend some time unpacking the concept of "overall company value".

Hidden within it we'll find the keys to successfully identifying and then advocating for a wide variety of technical investments -- and for understanding our work as engineers more broadly.

Also, it's fun![fn:: It's maybe *especially* fun if you have an obsessive love for developing a first-principles understanding of activities people are blindly doing all around you. Say.]

# Although some of the below may sound a bit theoretical, I'll *strongly encourage* you to really sink your teeth into it, really wrestle with how to apply it to your specific company.

# Why?

# Technical investments are fundamentally about *value*.

# As an engineering leader, you will be *extremely* well-served by having a full understanding of how to map from your company's highest level goals (aka, their plan for increasing overall value) all the way down to the work your team is doing, day-to-day.

# That full understanding is what will allow you to creatively and successfully advocate for work that no one is asking for.

# But it does take some slowing down and careful thinking to really see all those connections.

*** What? Oh, Good Lord No, This Is *Not* How You Talk To Stakeholders

Before we dig in: I'm *not* proposing you *lead* with all of the below, in your initial conversations with your stakeholders.

As you read the below just focus on understanding and applying the overall model of value *for yourself*.

Later in the book, we'll lay out a plan for gradually drawing your stakeholders into a repeated process of decision-making, which they will find delightful.

We'll do so in a way that doesn't require front-loading an economics lecture (stakeholders who are upset about their features being late are oddly resistant to economics lectures, I have found). [fn:: As Edmund Jorgensen and I first came to understand these ideas about company value (after reading the simply amazing Principles of Product Development Flow, by Don Reinertsen), we eagerly brought abstract models for value into just about every one of our discussions with stakeholders. We emerged from that experience a few years later, battered and bruised, with the very different approach you're going to see later.]

*** A Few Of My Favorite Misconceptions About Value

As step one to sharpening our understanding, let's list several things that are *not* reliable proxies for engineers increasing overall company value:

 - Cranking out new features as fast as possible

 - Rapidly chewing through all the well-groomed tickets on a team's sprint board

 - Writing "high-quality" or "defect-free" code

 - Living up to "commitments" to delivery deadlines

Stakeholders (and other engineering leaders!) will sometimes tell you, extremely confidently, that something on that list *is* all you need to worry about.

They might say: *"Stop asking so many questions and just do your job".*

Implicit in this exhortation is a belief that "engineering's job" is just one of those activities.

# There's a good chance that someone vaguely boss-like near has this belief.

Such a belief is fundamentally wrong.

Engineering's *job* is to create value for the business. Even if the people who hired you don't think about it that way. [fn:: This book is basically a ticket to the Dan Milstein Course in How To Give People What They Actually Want, Not What They Ask For, And Leave Them Very Happy Indeed]

Each of those activities is *potentially valuable* for a business... but, unfortunately, each of them is also *potentially damaging to value*:

Let's bring that to life with some examples.

Below are situations in which each of the above *is* valuable... and also situations where each of the above is *totally not* valuable:

 - *Cranking out new features as fast as possible*

   Super valuable when chasing product market fit and rapidly testing a series of new prototypes with customers.

   Super damaging when the company has built a product that customers fundamentally don't want or need, and the parade of new features is a desperate attempt to avoid facing that hard truth for as long as possible [fn:: I totally made this up I've never seen any stakeholders exhibit this exact behavior look something shiny.]

 - *Rapidly chewing through all the well-groomed tickets on a team's sprint board*

   Super valuable when the team is developing something genuinely important for the business, and the engineers and product leads are talking all the time, and the work is going live and driving learning every day.

   Super damaging if the product team is using the tickets as a way to avoid talking with the engineers and there's weeks of lead time to "write good tickets" (or god forbid Product Requirements Documents) and the engineers don't really understand the *why* of what they're building and the the team "closes tickets" by merging PRs into some infrequently-deployed branch.

   # Find Kellan quote about devolving to this?

 - *Writing "high-quality" or "defect-free" code*

   Super valuable if what the code does is genuinely important to customers, AND bugs in the code will cause those customers major problems, AND the code is going to live for a long time AND be changed by many engineers over the course of its life.

   Super damaging if the code has a high likelihood of being thrown away, and the time to write it "well" delays critical feedback from customers.[fn:: "The absence of bugs is not the presence of value" should be engraved on the wall of every academic institution that launches one of those stupid engineering productivity studies that measure defect rates as a form of "productivity", argh don't get me started]

 - *Living up to 'commitments' to delivery deadlines*

   Super valuable if, um... Um. Just give me a sec.

   Um.

   Look, making key business decisions based on extracting "commitments" from engineering is a fool's game for everyone involved.

   See Melissa Perri's excellent Escaping the Build Trap for much, much better options.

As the above makes clear, there's a lot of contextual nuance to understanding when your team's work is or is not valuable.

Fortunately, there is a unifying way to understand value, so that the most important factors of the context immediately pop into sharp relief.

And developing that unifying understanding will allow you to see a vast array of potential technical investments in a clear light.

So, now, we're ready for Part II, where we're going to *build a model!*

# Let's build a model!

# What's That You Say, You Want a Brief Detour Into Finance?
# Finance Is An Equation Built On Top of a Dream [Fantasy]
