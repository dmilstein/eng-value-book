:PROPERTIES:
:ID:       3DE23585-34F0-4C88-A16B-4558ACC45C99
:END:
#+title: Make Your Post-Mortems an Act of Visibility
#+filetags: :Chapter:
* Make Your Post-Mortems an Act of Visibility
** Intro

Over my first leading engineering at Ellevation Education, various folks on the team did an excellent job of identifying, advocating for and executing on all kinds of technical investments.

But, like any self-respecting software business, we still had some nasty bits of infrastructure that everyone was pretty nervous about.

At the top of the fear list was... Couchbase.

Ah, Couchbase.

We ran a clustered install of that key-value store, which served as a network-accessible, in-memory cache for our core application.

# Draconian, crouched like Smaugh atop a pile of gold, except, it was on the dreams and hopes of engineers

Why was the team so worried about Couchbase?

Let me count the (okay, just two) ways.

*** 1. Early 2000's-Era Magic Patterns, Argh

Ellevation's core product was written in C#/.NET, which, overall, had served us very well.

But, in the early days, a bunch of code had been written to take advantage of some Windows Framework automatic session store thing -- one of those "transparent" frameworks that tries to hide all the network calls, so the code can just live in the delusional belief that important in-memory objects are created when a user first logs in, and then never go away!

# persist across restarts, in the user's "session".

The magic session store things does this, of course, by serializing and deserializing binary representations of C# objects.

...??!

/me sighs, puts head in hands

What's that you say, that sounds super fragile, and just enormously likely to blow up due to version skew across both hosts and deployments?

Like building an apartment complex on top of an active minefield?

I HAVE NO IDEA WHAT YOU'RE TALKING ABOUT.

(ZOMG, I hate that pattern so much, aarggggghhh)

Ahem.

Now, that Couchbase cache held, among other things, a bunch of auth info for logged-in users.

That info was populated during login -- through, of course, the darkest of dark magics, since it was implicitly done through via user session setup in the magic Windows Framework.

It was pretty hard to understand the ins and outs of how that key data got written or read.

And even harder to safely change it.

At some point a few years back, several engineers had made an effort to replace the magic session with a new bucket in Couchbase, only written to by our own application code, not the magic framework.

But, as often happens with such efforts, they had only made it partway there -- and now we had two buckets, and a bunch of mysterious writes still happening to the magic bucket from... somewhere.

Any one of Ellevation's senior engineers would have told you two things:

 1) The way we used Couchbased was pretty problematic

 2) It wasn't going to be cheap to change it

But, wait, there's more!

*** 2. Lack of Operational Mastery

Couchbase was a finicky thing to get working across multiple nodes.

We didn't have great visibility into what the cluster was doing.

We struggled to regularly upgrade both the cluster and the client libraries.

There were lots of weird configuration options for how clients talked to the cluster.

Thanks to all of that (or maybe to fundamental truths of Couchbase? I honestly don't know), various restart patterns of client processes and/or Couchbase nodes made it possible for the whole system to get good and thoroughly wedged.

If you were to ask any of the CloudOps team who maintained that cluster if it would have been valuable for Ellevation to replace Couchbase as our cache, they would have first said "Absolutely", but would have immediately followed by saying "But there's no way we can do that."

By which they meant: there's no way the business is going to sign up for the amount of work involved.

** Risk & Potential Value

Let's see how we might make a case for the economic value of improving the cache access pattern and/or replacing Couchbase.

How could we lay out the potential impact on future profits to Bertha, our economically rational investor?

We'd say to her that Couchbase was creating a lot of *risk* for Ellevation.

It wasn't necessarily an active problem, on any given day.

But the thing that the engineers were concerned about was the *possibility* of two distinct "bad things" that *might* happen, at some future date:

**** Potential Downtime

if something were to go wrong, and customers couldn't login (or logged-in customers couldn't use the site) for some period of time, that would erode trust.

If that happened enough--or happened at a particularly bad time--it would impact renewals, and therefore profits.

**** Possible Development Roadblocks

Our Product team might discover that customers eagerly wanted--and were willing to pay for--new capabilities that required changes to auth.

If that were to happen, our struggle to safely change anything touching Couchbase would have made it difficult (or even impossible!) to build such capabilities.

Which would have meant paying a significant *opportunity cost*.

**** Making These Risks Visible

The more evidence Betha had for either the likelihood or the degree of badness of these Bad Things...

...the more she'd increase her estimate of future profits if Ellevation were to reduce either the likelihood or the degree of badness.

And thus the more company value that would be created.

# Also note that value can be created by making each of the Bad Things merely *less likely* to occur or *less bad* when it did occur.

# A Subtle Thing That Feels Important Enough To Pull Out: if Bertha has enough evidence then improvements in Couchbase would create value... *before any new capabilities were actually developed*.

# Value is *not* only created at the moment of launching new features (or collecting revenue for those features) -- it's created at the moment you improve the probabilistic estimate of the stream of future profits, made by an economically rational investor.

# aka, we don't have to fix everything about the caching system to create value.

So, how can we create visibility into the likelihood and the impact of these two possible Bad Things?

There is a somewhat profound difference between the two.

# XXX Maybe strip the below down to just the likelihood part?

For the Possible Development Roadblocks, the path to creating visibility into the likelihood and the degree of badness is pretty clear:

  a) Talk to a bunch of customers about the problems they have, and then

  b) Talk to the engineers about how hard it would be to build things that will solve those problems for customers

# At Ellevation, the product and sales and success teams did talk to customers, and, in fact, found plenty of potential value in adding capabilities that ran through auth (e.g. security/audit improvements, enterprise user management flows, etc).

# And the engineers could give some useful information about the challenges of working around Couchbase.

But for the Potential Downtime, it's a good bit trickier.

It's not like engineers could look at our poorly configured system and give a precise estimate of how likely it would be to take the entire site down just when Broward County, Florida was running their high-stakes annual English Learner FTE process.

Without *some* way to understand the degree of risk, engineers are reduced to just sort of waving their hands and saying "No really, this is bad!"

Which does not help anyone to make decisions.

Because, of course, there are plenty of other things that are *also* bad.

So which one do we pick to work on first?

How much effort should we spend in reducing the degree of badness in one particular area?

This is a super common challenge with tech investment opportunities.

Engineers often (and often, correctly!) worry about things with low likelihoods (aka, things that don't happen *often*), but very high degress of badness (aka, things that can destroy just a ton of overall company value).

In such cases:

 a) It often feels very hard to give a useful estimate of the likelihood

 and, as importantly,

 b) The full version of The Bad Thing likely hasn't happened "recently"... so it can be hard to create urgency to make investments *now*.

That latter point is really key.

If the *impact* (aka, Degree of Badness) of the bad thing happening is bad enough, it could very much be *economically rational* to decide to make some investments now, even if the *likelihood* is pretty low.

But humans deeply struggle with such decisions.

Sure, the whole site *might* fall over...

but it didn't fall over yesterday.

Nor the day before.

Nor any time in the last week!

And look, customers are just knocking down our door to fix all these bugs, and the product team has promised the CEO to demo the new feature at the QBR next week (and several PM's are feeling nervous about their next performance review, so they really want to nail that demo, etc, etc.).

And so the decision to invest in reducing the low-likelihood/high-impact risks gets put off... sometimes, forever.[fn:: Pop Quiz: if the site *does* later fall over, who will be blamed? Ding! Correct! The engineers. And thanks to the delightful magic of Hindsight Bias, someone might even say something like "How could you have possibly ignored that repeated error in the log files? Don't you care about the customer?"]

** Incidents As Useful Warning Signs From Reality

At Ellevation, we didn't have a precise estimate of the likelihood of Couchbase causing significant downtime.

But we had something that was, in some ways, even better.

Over the course of the last several years, we had experienced a slew of incidents, some large, some small, where issues with Couchbase had caused real downtime for customers.

Reality was offering us warning signs, about potential larger disasters with Couchbase, by dropping incidents onto our heads.

And, we had run careful post-mortems on just about every one of those incidents.

So we had a clear record of the pattern of Couchbase-caused instability.

From a certain perspective, you could understand the *goal* of a post-mortem as two-fold:

  1- Update your understanding of key risks, by extracting as much information as possible from an incident

  2- Based on that information, identify opportunities to reduce those risks


** Stakeholder Motivation

Digging in on the early warning signs will:

 a) Help determine current limits and bottlenecks, and

 b) Serve as an invaluable means of generating commitment from stakeholders

In some fantasy world, a CEO might find an estimate of a 5% chance of the site having a full day of downtime at a particularly bad time during renewal season extremely motivating.

In said fantasy world, the CEO would find that *more* motivating than the company experiencing a random 20 minute outage that irritated a few current customers.

But actual human beings, here on actual planet earth, hear "there's a 5% risk of a Bad Thing occurring" and think "Well, that sounds pretty unlikely" (no matter how Bad that Thing might be).

Whereas an outage that impacts a living, breathing customer is a vivid, near-moral failing -- even it was only for 20 minutes.

Of *course* the CEO is eager to make *some* investment to prevent such problems in the future!

Do you think the CEO doesn't care about the customer/the children??!

The moral framing short circuits people's normal decision making processes.

You're going to want to use this to your advantage, but do so very judiciously.

I strongly recommend *against* using the sort of post-incident shock and moral outrage as a prompt to launch the major replatforming that some engineers on your team have been campaigning for.

That effort is going to take way way longer than anyone can currently imagine, and there's a very real chance it *won't even help*.

Instead, I strongly recommend using the post-mortem findings to motivate investments to improve visibility - which can then makes later economic decisions more straightforward.

** Create Incidents To Create Visibility

E.g. Berlin story, or if things are painful, do them more often, game days, pinch tests, etc.

# Create Incidents For Fun and Profit

* Scraps/Thinking
Tell the story? Link to my videos/talks?

How much do I want to bring how I/we run post-mortems to life? I mean, *some* or people will have literally no idea what I'm talking about.

I do have "EN-How To Facilitate a Post-Mortem-310325-142830.pdf" in SavedEllevationFiles, which is pretty far along the path to a write up on how to run them. Maybe shove that in an appendix.


Theory: post-mortems make risks visible. They are early-warning signs.

How much advice do I give on actually running post-mortems?

Where did we get lucky?

Examples:

 - Site fell over because a change to auth locked all users out

   Risk = hard to safely change auth code, poor testing, monitoring

 - System locked up under load

 - Customer deleted a bunch of data

 - Team deleted a bunch of data

** Thinking <2025-07-09 Wed>
I think *don't* explain how to run a good post-mortem (maybe throw in an appendix)

*Do* explain what the *outcome* of a good post-mortem is.

And the point of this chapter is how to *use* that outcome effectively.

Tell the story of Roberto + Vahe?

Repeated failures of Couchbase at Ellevation

HubSpot -- the customer happiness crisis.

I can sort of imagine two ways to start:

1- I'm focusing on an incident, and maybe it's the moment of wrapping up the post-mortem.

2- I'm focusing on a risk/concern of engineers, and then talking about how to use post-mortems as a way to make that visible.

I have a bunch in [[id:2EC03879-2A23-4546-BCB8-E9A464665A03][Turn Concerns Into Potential Value]] about this. Almost the germ of this chapter.

What's the core takeaway from that chapter, the thing I want them to do differently?

Stop thinking about post-mortems as "for engineering" and think about an *output* of a post-mortem as "visibility and/or a story engineers can tell stakeholders".

And then some tactics for that.

** Bertha and the Risks


We could say to her:

"The combination of the pattern of use along with the operational challenges makes it incredibly hard to safely change anything related to auth. Thus, if we want to add new forms of auth, to either meet new security concerns, or to break parts of our app into services that share tokens in new ways, it will be very hard, or even impossible to do so."

She might well ask: "Do you expect to need to do either of those things, over the next few years?"

To which we would have said: "Actually, yes. Enterprise customers are wanting fuller Single Sign On connections + a set of security improvements that run right through auth. Plus we need to move some work to async processing, which is very hard right now, since all the request paths assume they'll get a user token."

So she'd say: it sounds like that might be a worthwhile investment, even if sizeable.

The only problem: our CEO was not Bertha.

To be clear, Ellevation's CEO and Ellevation's Head of Product were both extraordinarily willing to listen to engineering. But they were both also trying very hard to achieve a bunch of product wins, in order for the business to keep growing.

And there plenty of other problematic areas.

And this was not a problem which was natively visible.

How can you make this kind of ugliness and operational toil more visible?

* Possible Arc
** Story of value opportunity which is opaque

Maybe, specifically, Couchbase @ Ellevation?

Hold back the repeated failures, just talk about the nervousness.

Maybe even misdirect slightly -- the way the keys was used was weird, there were strange bits tied directly into magic sessions, etc.

But, like, deeply tied into auth{n,z}, all kinds of stuff.
** Hard to motivate investment -- scary to change.

** Maybe: bridge to, this is a common problem w/ tech investments

Illustrate with a bunch of other things from my list.

** Making risk both visible and immediate (aka, concrete, non-theoretical)
Those are subtly different.

** Return to story: Couchbase implicated in lots of outages

** Typical: how complex systems fail
Many (not all) risks make themselves known through small failures.

** We ran Post-Mortems on outages, and *had product in the room* + took time to write up results

** Thus, eventually, Jeremy, (Ben? Ryan? Kiwis?) moved to ElasticCache
Motivating the investment wasn't hard - because the risks of downtime were *visible* thanks to the post-mortems serving as an early warning system

** What Post-Mortems Must Output, to Make This Work
A human-readable summary linking overall customer and business goals to the outage.

You don't have to have everyone read that summary, but you need it.

And you likely want your "nearby" stakeholders to participate -- e.g. Product.

You can think of the *goal* of a post-mortem as two-fold:

  1- Create a picture of a current state of risk

  2- Identify opportunities for improvements, to reduce that risk

The key pitch I'm making is that Goal #1 can and should be used *outside of engineering*. Both so that immediate investments are easier to motivate, but also so that long-term stories of risk can be made clear.

** Running Good Post-Mortems Left as Exercise For Reader

jk, here's a link, here's an appendix.

** Examples of Risks

*** Capacity

*** Stability

*** Deploy Friction

*** Data Inconsistency (e.g. Inventory Variance)

*** Fragile Architectures (e.g. Async Kafka Storm)
