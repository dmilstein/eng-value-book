:PROPERTIES:
:ID:       3DE23585-34F0-4C88-A16B-4558ACC45C99
:END:
#+title: Make Your Post-Mortems an Act of Visibility
#+filetags: :Chapter:
* Make Your Post-Mortems an Act of Visibility
** Intro

I joined Ellevation Education in 2020. Over my first few years there, various folks on the team did an excellent job of identifying, advocating for and executing on all kinds of technical investments.

But, like any self-respecting software business, we still had some nasty bits of infrastructure that everyone was pretty nervous about.

At the top of the fear list was... Couchbase.

Ellevation ran a clustered install of Couchbase, serving as a network-accessible, in-memory cache for our core application.

# Draconian, crouched like Smaugh atop a pile of gold, except, it was on the dreams and hopes of engineers

Why was the team so worried about Couchbase?

Let me count the (okay, just two) ways.

# Ellevation, like any self-respecting software business, had some bits of infrastructure that the team was just continually frustrated by.

# Not long after I joined in 2020, it became clear that near the top of that list was the Couchbase cluster.

# We were using Couchbase as network-accessible in-memory cache.

# These are both real potential value opportunities -- good opportunities for technical investments.

*** 1. Crappy Early 2000's Magic Patterns, Argh

Ellevation's core product was written in C#/.NET, which, overall, had served us very well.

But, in the early days, a bunch of code had been written to take advantage of some Windows Framework automatic session store thing -- one of those "transparent" frameworks that tries to hide all the network calls, so the code can just live in the naive belief that key in-memory objects are created when a user first logs in, and then never go away!

# persist across restarts, in the user's "session".

The Windows Framework automatic session store things does this, of course, by serializing and deserializing binary representations of C# objects.

...??!

/me sighs, puts head in hands

What's that you say, that sounds super fragile, and just enormously likely to blow up due to version skew across both hosts and deployments?

Like building an apartment complex on top of an active minefield?

I HAVE NO IDEA WHAT YOU'RE TALKING ABOUT.

(ZOMG, I hate that pattern so much, aarggggghhh)

Now, that Couchbase cache held, among other things, a bank of auth info for logged-in users.

That auth info was populated as part of login -- of course, through the darkest of dark magics, since it was implicitly done through some session setup step of the magic Windows Framework.

It was pretty hard to understand the ins and outs of how that key data got written or read.

And even harder to safely change it.

Several engineers had made an effort, at some point a few years back, to replace the magic session with a new bucket in Couchbase, which was only written to by our own application code, not the magic framework.

But, as often happens with such efforts, they had only made it partway there -- and now we had two buckets, and a bunch of mysterious writes still happening to the magic bucket from... somewhere.

Any one of Ellevation's senior engineers would have told you two things:

 1) The way we used Couchbased created a lot of risk

 2) It wasn't going to be cheap to change it

But, wait, there's more!

*** 2. Lack of Operational Mastery

Couchbase was a finicky thing to get working across multiple nodes.

We didn't have great visibility into what the cluster was doing.

We struggled to regularly upgrade both the cluster and the client libraries.

There were lots of weird configuration options for how clients talked to the cluster.

Thanks to all of that (or maybe to fundamental truths of Couchbase? I honestly don't know), various restart patterns of client processes and/or Couchbase nodes made it possible for the whole system to get good and thoroughly wedged.

If you were to ask any of the CloudOps team who maintained that cluster if it would have been valuable for Ellevation to replace Couchbase as our cache, they would have first said "Absolutely", but would have immediately followed by saying "But there's no way we can do that."

By which they meant: there's no way the business is going to sign up for the amount of work involved.

** Risk & Potential Value

Can we make a case that the engineers could have created economic value for Ellevation by some combination of changing the cache access pattern and/or replacing Couchbase itself?

If so, how exactly?

How might we lay out the potential impact on future profits to Bertha, our economically rational investor?

We'd say to her that Couchbase was creating a lot of *risk* for Ellevation.

It wasn't necessarily an active problem, on any given day.

But the thing that the engineers were concerned about was the *possibility* of two distinct "bad things" that *might* happen, at some future date:

**** Risk of Downtime

if something were to go wrong, and customers couldn't login (or logged-in customers couldn't use the site) for some period of time, that would erode trust.

If that happened enough--or happened at a particularly bad time--it would impact renewals, and therefore profits.

**** Risk of Development Roadblocks

Our Product team might discover that customers eagerly wanted--and were willing to pay for--new capabilities that required changes to auth.

If that were to happen, our struggle to safely change anything touching Couchbase would have meant paying a significant *opportunity cost*.

In that world, making it easier to make auth changes would increase Bertha's probabilistic estimate of future profits, and therefore create value.

Of course, the more evidence Betha had that customers really wanted (and would pay for!) capabilities locked up behind Couchbase, the more her estimate would go up, and thus the more value that would be created.

A Subtle Thing That Feels Important Enough To Pull Out: if Bertha has enough evidence of customer demand *and* enough evidence of genuine roadblocks in the current setup, then easing the path of development would create value... *before any new capabilities were actually developed*.

Value is *not* only created at the moment of launching new features (or collecting revenue for those features) -- it's created at the moment you improve the probabilistic estimate of the stream of future profits, made by an economically rational investor.

Also note that value can be created by making each of the Bad Things merely *less likely* to occur.

We don't have to create a perfect caching system to create meaningful value.

**** Making These Risks Visible

There is a somewhat profound difference in the challenge of making these two risks visible.

For the Risk of Development Roadblocks, the path to making the risk visible is pretty clear: go and talk to a bunch of customers.

At Ellevation, the product and sales and success teams did this, and, in fact, found plenty of potential value in adding capabilities that ran through auth (e.g. security/audit improvements, enterprise user management flows, etc).

But for the Risk of Downtime, it's a good bit trickier.

It's not like engineers could look at our poorly configured system and give a precise estimate of how likely it would be to take the entire site down just when Broward County, Florida was running their high-stakes annual English Learner FTE process.

Without *some* way to understand the degree of risk, engineers are reduced to just sort of waving their hands and saying "No really, this is bad!"

Which does not help anyone to *make decisions*. Because there are plenty of other things which are *also* bad -- so which one do we pick to work on first? How much effort should we spend in reducing the degree of badness?

This is a super common challenge with tech investment opportunities.

There is some risk of an "occasionally very bad thing" happening -- but that very bad thing doesn't happen often, so:

 a) It feels hard to give a useful estimate of the risk of it happening

 and, as importantly,

 b) It may not have happened "recently", so it's very hard to generate urgency to deal with it now instead of later.

That latter point is key -- if the bad thing is bad enough, it's economically rational to deal with it *now* and not later.

But humans struggle to make decisions that way.

Sure, the whole site *might* fall over, but it didn't fall over yesterday, or the day before, and look, we have customers knocking down our door to fix all these bugs, and the product team has promised the CEO to demo the new feature at the QBR next week, so we'll deal with the downtime risks *later*. I'm sure next week we'll find time?

Of course, if the site *does* fall over, who will be blamed? Yep, the engineers.

And thanks to the delightful magic of Hindsight Bias, everyone will feel like "How could they have possibly ignored that repeated error in the log files? Don't they care about the customer?"



How could we show that the infra setup itself was risky?

If this had been a "small" investment, we could have just based a case on adding flexibility around auth.

But, like many important tech investments, this wasn't small.

** Risk And Visibility

One way to understand it is that the software had a lot of downside risk.

* Scraps/Thinking
Tell the story? Link to my videos/talks?

How much do I want to bring how I/we run post-mortems to life? I mean, *some* or people will have literally no idea what I'm talking about.

I do have "EN-How To Facilitate a Post-Mortem-310325-142830.pdf" in SavedEllevationFiles, which is pretty far along the path to a write up on how to run them. Maybe shove that in an appendix.


Theory: post-mortems make risks visible. They are early-warning signs.

How much advice do I give on actually running post-mortems?

Where did we get lucky?

Examples:

 - Site fell over because a change to auth locked all users out

   Risk = hard to safely change auth code, poor testing, monitoring

 - System locked up under load

 - Customer deleted a bunch of data

 - Team deleted a bunch of data

** Thinking <2025-07-09 Wed>
I think *don't* explain how to run a good post-mortem (maybe throw in an appendix)

*Do* explain what the *outcome* of a good post-mortem is.

And the point of this chapter is how to *use* that outcome effectively.

Tell the story of Roberto + Vahe?

Repeated failures of Couchbase at Ellevation

HubSpot -- the customer happiness crisis.

I can sort of imagine two ways to start:

1- I'm focusing on an incident, and maybe it's the moment of wrapping up the post-mortem.

2- I'm focusing on a risk/concern of engineers, and then talking about how to use post-mortems as a way to make that visible.

I have a bunch in [[id:2EC03879-2A23-4546-BCB8-E9A464665A03][Turn Concerns Into Potential Value]] about this. Almost the germ of this chapter.

What's the core takeaway from that chapter, the thing I want them to do differently?

Stop thinking about post-mortems as "for engineering" and think about an *output* of a post-mortem as "visibility and/or a story engineers can tell stakeholders".

And then some tactics for that.

** Bertha and the Risks


We could say to her:

"The combination of the pattern of use along with the operational challenges makes it incredibly hard to safely change anything related to auth. Thus, if we want to add new forms of auth, to either meet new security concerns, or to break parts of our app into services that share tokens in new ways, it will be very hard, or even impossible to do so."

She might well ask: "Do you expect to need to do either of those things, over the next few years?"

To which we would have said: "Actually, yes. Enterprise customers are wanting fuller Single Sign On connections + a set of security improvements that run right through auth. Plus we need to move some work to async processing, which is very hard right now, since all the request paths assume they'll get a user token."

So she'd say: it sounds like that might be a worthwhile investment, even if sizeable.

The only problem: our CEO was not Bertha.

To be clear, Ellevation's CEO and Ellevation's Head of Product were both extraordinarily willing to listen to engineering. But they were both also trying very hard to achieve a bunch of product wins, in order for the business to keep growing.

And there plenty of other problematic areas.

And this was not a problem which was natively visible.

How can you make this kind of ugliness and operational toil more visible?

* Possible Arc
** Story of value opportunity which is opaque

Maybe, specifically, Couchbase @ Ellevation?

Hold back the repeated failures, just talk about the nervousness.

Maybe even misdirect slightly -- the way the keys was used was weird, there were strange bits tied directly into magic sessions, etc.

But, like, deeply tied into auth{n,z}, all kinds of stuff.
** Hard to motivate investment -- scary to change.

** Maybe: bridge to, this is a common problem w/ tech investments

Illustrate with a bunch of other things from my list.

** Making risk both visible and immediate (aka, concrete, non-theoretical)
Those are subtly different.

** Return to story: Couchbase implicated in lots of outages

** Typical: how complex systems fail
Many (not all) risks make themselves known through small failures.

** We ran Post-Mortems on outages, and *had product in the room* + took time to write up results

** Thus, eventually, Jeremy, (Ben? Ryan? Kiwis?) moved to ElasticCache
Motivating the investment wasn't hard - because the risks of downtime were *visible* thanks to the post-mortems serving as an early warning system

** What Post-Mortems Must Output, to Make This Work
A human-readable summary linking overall customer and business goals to the outage.

You don't have to have everyone read that summary, but you need it.

And you likely want your "nearby" stakeholders to participate -- e.g. Product.

You can think of the *goal* of a post-mortem as two-fold:

  1- Create a picture of a current state of risk

  2- Identify opportunities for improvements, to reduce that risk

The key pitch I'm making is that Goal #1 can and should be used *outside of engineering*. Both so that immediate investments are easier to motivate, but also so that long-term stories of risk can be made clear.

** Running Good Post-Mortems Left as Exercise For Reader

jk, here's a link, here's an appendix.

** Examples of Risks

*** Capacity

*** Stability

*** Deploy Friction

*** Data Inconsistency (e.g. Inventory Variance)

*** Fragile Architectures (e.g. Async Kafka Storm)
