:PROPERTIES:
:ID:       03D1870C-E583-4D5C-9589-5E0799793D48
:END:
#+title: Offer Decisions, Options & Off-Ramps
#+filetags: :Chapter:

* Offer Decisions, Options & Off-Ramps
** Intro

# Build Milestones Around Decisions
# Offer A Next Decision Point
# Offer a Timeboxed Next Decision Point
# Maybe retitle "Set Milestones to Enable Decisions"

Let's review where things ended, in our last chapter, for the engineering leader we featured there.

She worked with her engineers to dig into various concerns.

As she did so, she consulted in her mind with Bertha, her economically rational investor (slash imaginary friend), to understand how addressing each of those concerns could potentially increase a probablistic estimate of future profits -- and therefore create value for the business.

With Bertha's help, she narrowed down the list of opportunities to a single leading contender: improving site performance (which the engineers had initially described as "the database seems unhappy").

She took time to practice the skills of tactical empathy, and then deployed those skills on building rapport with her key stakeholder.

Then, we she met with that stakeholder to pitch an investment in creating visibility into site performance, she spoke to both her stakeholder's fears (/"worst case, many of those pages could potentially seem broken."/ ) and their hopes (/"guarantee an excellent experience for the first Enterprise customers who commit to Forms 2.0."/)

She offered some initial visibility that she and her team had built "on the side" as a means to understand why investment is indicated now and not later (/"Enterprise customers who use the old, legacy Forms 1.0 product, all have *much* bigger data sets than we've seen for the new Form 2.0 in production so far"/)[fn:: This is a very useful form of visibility -- the use patterns of customers in different segments should absolutely inform investment.]

She focused her initial pitch for investment on building further visibility. (/"We'd like to propose that we commit time to building a clearer picture of our capacity limits"/)

# XXX Mix in the basic DB server CPU monitoring? If so, mix it back into previous chapter.

To her surprise and delight... her stakeholder was convinced.

But then, before the enginering leader could feel good for even a minute, the stakeholder asked: /"How long will this project take?"/

And then promptly blew right past the proposed focus on visibility, and instead demanded a time estimate for achieving the ultimate outcome:

/"How long will it take to ensure good performance for enterprise users?"/

What can our engineering leader friend say?

She has *literally no idea* how long it would take to improve performance for enterprise users -- she doesn't actually even know yet if there's a performance issue for enterprise users yet! That's why she's proposing an investment in building visibility!

But her stakeholder is staring expectantly at her.

** Some Supposedly Fun Ways To Estimate That I'll Never Try Again

# Some Options I Can *Not* Recommend When Asked For An Estimate For a Tech Investment

# Some Supposedly Fun Ways To Estimate Tech Investments That I'll Never Use Again

Yep, I've tried all of these and have the battle scars and failed projects to show for it.

*** Guess and Pad

Often, when you go to propose a technical investment, your engineering team will have a hunch about what they need to do.

Except they won't call it a hunch.

They'll say:

/"We totally *know* what the problem has to be. We just need some time to fix it. We can get it done in a week, easy."/

Engineers (who I love! Because I am one!), will say this with perfect confidence...

...even if they actually have absolutely *zero* understanding of what is causing a problem.

Honestly, I think it's just part of being an engineer -- we *know* that one part of the code was written in a dumb way, and now there's an issue, like, /near/ that code, it's *obviously* caused by it. We can already visualize how we want to clean up the code, and how much better things will be once we've done so.

If your team announces they know how to fix a problem...

...but they have *not* first built visibility...

...and/or have *not* done some fairly deep exploratory work...

...I strongly recommend that you just *nod and smile*.

/"Oh, yes,"/ you say, /"that sounds *extremely* likely. I can't wait to dig in!"/

But under no circumstances should you turn around and use this to scope a project with your stakeholder.

You might think to yourself: "I'm no fool. I'll *pad* that estimate. That way, even if the engineers were a little optimistic, we'll still get it done".

# Mama didn't raise no foolish engineering manager

You might say to your stakeholder "The team knows what to do, we just need three weeks."

And pat yourself on the back for the 3x padding, in going from one week to three.

Ohmygod, please don't do this.

Look back at our running example, which is typical of so many potential technical investments: there's a *risk* of bad performance for enterprise customers.

Let's say your team tells you their certain they know what the problem is.

They've long been unhappy about how much data gets shipped to the browser.  They are *convinced* Enterprise customers are going to take a hit because they have much larger data sets. It's time to implement pagination.

But the team has *not* measured overall performance for anyone, they haven't profiled requests in production, they have no load tests, etc.

What might the team find in those three weeks of work, when they go to add pagination to only send smaller chunks of data in response to each request... which they "know" will make things much better?

Well, if you're *extraordinarily* lucky, they'll be able to build sufficient visibility into actual enterprise performance, *then* apply the fix they "knew" was the problem, then verify that it worked, then share the results with your stakeholder.

But that, um, almost never happens?

Instead, you'll essentially always get hit by some combination of:

 - *It's harder to build visibility than the team expected*

   E.g. user-perceived load time is driven both by server-side response time and client-side render time... and it turns out to be deeply non-trivial to link those two up.

   Or the team adds detailed monitoring to production, but customers only do the "heavy" actions once in a while, so there's no visibility yet.

   Or the team tries to reproduce issues outside of production, but everything performs suspiciously well. They think they need some form of concurrent load, but they haven't got that implemented yet.

 - *The visibility, once built, shows the the problem isn't where the team thought it was*

   Maybe it turns out that the database is struggling to generate responses for Enterprise customers at all, so the pagination idea makes things *worse*, not better (because the system now hits the database every time the user advances a page, instead of once up front).

   Or, it's something that literally no one was thinking about -- e.g. the team discover there's a series of weird hops through various gateway servers, and one of those gets transiently overloaded and then slows everything down for a period. And it's been hitting all customers, but only the Enterprise customers have enough clout for the team to hear about it.

 - *Fully addressing the actual problem turns out to be much harder than anyone hoped*

   The team discovers that some engineer early in the company's history re-invented the Entity Attribute Value pattern, and half your customers are storing arbitrary data in implicit per-customer schemas, which are literally impossible to sanely index.

# XXX What the hell is the name of that thing? Is Entity Attribute? Entity Attribute Value?

If almost any of those Bad Things happen (and you're just about guaranteed that one will), you'll find yourself, when your three weeks are up, in a very tough spot.

Your stakeholder is expecting the "engineering issues" to be resolved, and to now "get back" the team's full capacity to focus on "business needs".

That is a crappy, crappy place to be.

*** Give a Brief Primer on Company Value & Information Theory

Yeah, sigh. I so wish this worked.

If you pull this off, please let me know, and I will buy you all the beers to learn more.

*** Refuse Outright

You could simply tell the stakeholder:

/We have literally no idea how long this will take./

I mean, this is... *true*, but it's not exactly *helpful*.

Your stakeholder isn't just asking for an estimate to be a jerk, they're asking because the new information you've shared, as part of your pitch for a tech investment, has *created a new problem for them*.

And, ideally, you want to help them solve that problem.

** Understand The Challenges Facing Your Stakeholder

# When a stakeholder learns for the first time about a potential technical investment, they suddenly acquire some new, tricky problems.

# Thanks to the information you've shared with them, your stakeholder has some new problems.

When a stakeholder considers committing to a technical investment, they suddenly acquire two new decisions to make, and one new fear to manage:

 - *Decision 1: how can they make time to /start/ this work?*

   E.g. if the team is going to investigate performance issues in this next sprint, what planned stories or epics is the stalkeholder going to have to deprioritize?


 - *Decision 2: what should they tell Other People about the impact on Other Goals?*

   If it's going to take more than a sprint or two to improve Enterprise performance, the team is going to have to drop *something* significant to make time.

   e.g. maybe there's that complex new data view that the Enterprise customers have been clamoring for.

   Your stakeholder has been expecting to make real progress on that, over the next few months.

   They now have to decide if that's at risk -- and if *is*, they need to go and socialize that fact across your company.

   If the technical investment ends up taking real time, your stakeholder is going to have disappoint *someone*. And they have likely learned that it's better to tell people bad news sooner than later.


 - *Fear: if the work expands in scope, they will lose all control and won't be able to achieve their own goals*.

   One of a stakeholder's biggest fears is that they'll end up stuck in some endless engineering-driven project, with *no way to stop*.

   Any stakeholder who has worked with engineering has had at least one really unpleasant experience of estimates blowing up beyond all expectation, and finding themselves just mired for months and months, unable to resume forward motion.

   Having that happen for a technical investment feels extra terrifying -- because the stakeholder may not feel like they understand options for shrinking scope.

Ultimately, the reason they're asking for an estimate is to be able to make those decisions and to manage that fear -- *even if that's not how they're consciously thinking about it*.

If they had a good estimate stretching out into the future, they could *both* set expectations with Other People, *and* also hold the engineering team accountable to wrapping the investment up in a reasonable time frame.

Unfortunately, as per the previous section, for most technical investments, you won't, at this point, be able to give your stakeholder a sufficiently accurate estimate to allow them to solve these (very real!) problems.

But, because we can understand those underlying issues, we can find a way to help the stakeholder, *without committing to a false or damaging estimate*.

Before we walk through how to do so, I want to talk for a moment about how your stakeholder manages "medium-term" goals -- because those medium-term goals are what they're updating, in making Decision 2 ("Who do they need to tell about this shift?").

If you have an even vaguely rational stakeholder, they won't think of their medium-term goals as completely rigidly fixed[fn:: If your stakeholder is *not* at all flexible, and is running your work purely using the rigid tools of 'project management', then, first, siiiiigh. Second, I strongly suspect that, for your company to have any shot at winning, someone will need to bootstrap a product management function. Doing so is beyond the scope of this book -- check out Melissa Perri's truly excellent Escaping the Build Trap for ideas.]. Instead, they'll have some mental model of:

 - The order in which they're going to get to various goals

 - Key risks that could delay various goals

 - Which other people need to be told about changes to which goals, and when

A core part of your stakeholder's job is using this ever-evolving medium-term picture to *collaborate* with people across the company -- be that conducting user testing, planning rollouts, preparing sales materials, or the like.

All of those are *genuinely important* business actions -- and all have genuine lead-time challenges (meaning, the company needs to start preparing for them well in advance -- so having no idea when engineering will finish work injects all kinds of expensive delays).

With our eye on those collaborations, we can find creative ways to help our stakeholder to make their key decisions and manage their fear.

Let's see how.

# They're asking for an estimate that stretches out past the next few weeks to as part of preparing both of these decisions.


** Timebox to a Decision, Options & Off-Ramps

Let's illustrate this with our running story, from the last chapter.

As a reminder, the engineering leader wrapped up the initial part of her pitch by saying:

/Fortunately, we think we have a couple of good options for speeding things up -- once we find any bottlenecks./

/Unfortunately, we don't have great *visibility* into how those pages are performing, or where bottlenecks are./

/Therefore, we'd like to propose that we commit time to building a clearer picture of our capacity limits, and, once we've done that, ensuring that we have sufficient capacity to guarantee an excellent experience for the first Enterprise customers who commit to Forms 2.0./

A useful way to make that concrete, and to start to help the stakeholder make their key decisions, would be to say something like:

/We're proposing that Andrea spends the next three weeks developing and implementing a first draft of Service Level Indicators -- which will show us, basically "Are customers on Forms 2.0 *using it successfully*?". And if they're *not* -- if they do hit performance issues -- we can know *before* the help desk or success team comes to us, and we believe the team can quickly swarm and address it./

/Building SLI's has some real tradeoff costs. We had planned for Andrea to take point on adding new features to the Search Indexing. We think the potential performance issues are a bigger risk. And, if we understand it right, not all the ENT customers need those new Search features, so we might be able to delay the transition for customers who do need them. We believe the risk of bad performance cuts across all of the ENT customers./

/The key milestone Andrea would be working towards, which she can hit within 3 weeks, would be to ready to sit down with you and me, and review both the definitions of an initial set of the SLI's *and* how those SLI's are performing in production./

/We'd then be able to decide, together, if that performance seems acceptable. If so, we can return to our original plans and just keep an eye on performance as transitions ramp up. If performance already seems problematic, or if we're just uncertain, the team could start some proactive load testing, or if we've found any bottlenecks, deal with those./

/But we don't have to make that decision yet -- we'll have more info in just a few weeks./

Note how the engineering leader is offering a carefully time-boxed increment that ends with a *shared decision*

The milestone is, in essence: build a thing in no more than 3 weeks that will *both* create some incremental improvements *and* allow the stakeholder and the eng leader to make a collaborative decision about what to do next.

This offer helps the stakeholder with *both* of their problems.

For the short-term, the engineer leader has given them a precise bound on how much capacity they'll need -- it's three weeks for Andrea, no more, no less (and, in our scenario, the engineering leader offered some creative brainstorming about how to free up Andrea's time -- that's a very nice bonus if you can do it).

For the medium-term, the engineering leader has done two things.

First, they've forecast a couple of *possibilities*:

 - Spending time on proactive load testing

 - Digging in and remediating bottlenecks

 - Returning to planned work but "keeping an eye" on performance

Such *potential follow ups* allow a stakeholder to update an evolving picture of the medium-term.

They might ask follow up questions to flesh out their understanding (e.g. "What might be involved in load testing?"). I have generally found those to be very productive conversations.

The second thing the engineering leader has done is to *set a next decision point*:

 - In three weeks, we'll meet, we'll have more information, and we can decide what to do next..

Knowing when that next decision point is coming allows the stakeholder to make concrete decisions, right now, about who to communicate with immediately, who to wait until after they hit that decision point, etc.

Let's now flip it around and see if from the engineering side.

We've defined a clean milestone, with an *outcome* of:

*Support a decision about "what do we do next?"*

An engineer working towards such a "decision outcome" has a great deal of flexibility to adjust scope.

E.g. if Andrea discovers that adding the SLI's is more work than expected, she can shrink her scope to the 1-2 very most important ones and bring those to a conversation with the engineering leader and the stakeholder.

That would allow the engineering leader to have precisely the conversation with the stakeholder that they promised: we have this initial visibility, we can now decide if it's sufficient, and what next steps we want to take.

Also, such a decision outcome hopefully prompt the engineers to ask:

/What information will the engineering leader and stakeholder need to make this decision?/

Which is a *great* question for them to ask.

** Decision Framing: Examples

# Challenges in Estimating Tech Investments

Let look at some classic areas of technical investment.

For each, we'll set it up where the decision point may feel tricky, and then we'll offer some ideas.

# XXX make prose less awful

*** Scenario 1: Unreliable Deploys

The deploy pipeline randomly fails for no clear reason.

When the team does successfully deploy, they frequently end up emergency reverting in exciting ways.

You and the team have developed some rough initial visibility showing that engineers are starting to deploy less frequently as a result.

That plus a heavy dose of Accelerate has your stakeholder ready to talk.

But, if you want to make *any* kind of estimate, you're in a tough place.

Because there is a simply *huge* range of possible problems that could be causing unrelaible deploys.

It could be anything from some trivially misconfigured CI/CD parameters, which you can fix in a few days, all the way to fundamental architectural choices that are causing genuine conflicts between multiple teams, which could take several years of work to fix.

Thus, the "next decisions" that you and the stakeholder need to make will be best supported by three progressively stronger forms of visibility:

 - How bad are the deploys, overall? (aka gather the Accelerate metrics)

 - Once we know which facet of deploys is bad, what is the underlying problems?

 - What are the options to address that problems?

You can progressively move through that list, at each step, offering a next decision point around the next level of visibility and a forecast of some of your current guesses of what might come next.

E.g. partway through that might look like:

"We're okay on lead time and deploy frequency but we have a major problem with revert frequency.

Over the last month, engineers had to rollback just short of 20% of their deploys. That's  huge drag on velocity *plus* an unpleasant customer experience.

We strongly recommend spending some time, now, to see if we can improve that.

I've talked to the team, and they think there are a couple of buckets those reverts fall into, based on what breaks when they deploy.

One bucket is "bugs in the navigation masthead", which has weird knowledge of all the separate systems. The team has done some work on the masthead in the last year, if we have to dig in here, we should have some reasonable options.

Another bucket, unfortunately, seems to be issues in the ancient legacy reports, which no one knows how to safely change. If that turns out to be a major driver of problems, we'll have to dig in to learn more.

In any event, starting today, we're going to spend a bit of extra time *categorizing* all the reverts and also checking in about them during standups every morning. That way, we can review what we've learend in two weeks, and talk about options for next steps at that point."

*** Scenario 2: Nasty Legacy Code

Your product has a couple gnarly old features that only your earliest customers still use -- and, bonus, they're written on their own Very Special tech stack, that you're not using anywhere else.

But guess what: your CEO *personally* acquired those customers in the early days of the business. And he is *oddly reluctant* to retire those hoary old product features and run the risk of pissing off and potentially churn those Very Special early customers.

Meanwhile, your product team visibly seethes at the idea of taking months to move those old, barely-used-by-anyone features to the current tech stack.[fn:: I mean, *obviously*, this is actually a conflict between the CEO and the Product team. But at a wild guess from here in the footnotes, at *your* company they've both somehow succeeded in convincing themselves that this is an engineering issue. At a guess.]

Your engineers have been growing more worried and frustrated.

Then, you realize there is a way to make the potential for value much more visible.

Your company strategy is to move upmarket, and start selling to enterprises.

Early sales conversation have made it clear that those enterprises are going to demand serious attestations of security -- third party audits, penetration tests, architecture reviews.

That nasty old legacy part of your product is just a festival of security issues -- out of date libraries, wide open permissions for operators, that horrible password reset page that can be compromised by running View Source, long-abandoned JS frameworks, you name it.

Your product team finds this argument persuasive.

While looking over the long list of new features they're *also* trying to build for those enterprise customers, they say:

"I can't just tell the CEO that we're killing his baby. To make this case, can you please come up with a good estimate of how much time it would take deal with the worst of the security issues? Either by fixing in place, or porting to our current architecture?"

Again, what do you say?

It's a murky mess, not only do you not know what the worst security issues *are*, you don't even have half an idea on how hard it would be to fix just about any of them.

Your team barely understands this system, they certainly can't give you meaningful estimates of the time to do... things you can't yet specify?

What if you try to be conservative and give a "big" estimate... but then the CEO says "Yes, do it!"? then what the heck are you gonna do?


*** Moar Scenarios...

You propose enabling parallel development across multiple teams by inserting an interface layer in the middle of some convoluted mess of legacy code. Product is ready to consider saying yes, but asks "How long will that take?", before they commit.

You propose creating tooling to allow the help-desk to fix a slew of data issues that are currently requiring your team to spend hours a week executing SQL updates by hand. You guessed it -- your stakeholder would be happy to say yes, if they knew how long it would take.

Your backup regimen hasn't been reviewed in a while, you've raised sufficiently economic fear for your stakeholder to agree to some work to verify safer backups -- how long will that take?

* Scraps
Your stakeholders will almost definitely see them in that light, certainly at first.

You or your team may also do so -- especially if you've been forced to operate in a scarcity mindset around engineering-driven work. E.g. if you're able to bargain your way into three weeks of "engineering" work in any year, you'll be forced to think "What 'project' will fit into those three weeks"



Tech Investments should absolutely be broken up into defined steps, and each of those steps should almost always should have a clean end date (and therefore be forced to shrink in scope, as that date approaches).

Aka, you and your team should live and breathe timeboxing, when you go to execute on your investments.


Investments are, ideally, "done" when it no longer make sense to continue to invest.

And even then they're not really "done", so much as other things become, for that moment, more valuable to invest in.


Why?

Not just because you don't know how much work is involved, but, more fundamentally, because you don't know when you'll reach a point where the potential value of further tech investment is less than the value of other investments (aka the opportunity cost).

Not only is that hard to even estimate up front, it will *change over time*.



** Old Intro

Things are going well.

You've worked with your engineers to turn concerns into potential value.

Bertha, your economically rational investor/imaginary friend, has helped you identify forms of value based on probablistic estimates of future profits.

You've narrowed down your list of opportunities to the one you think has the greatest potential value for your business.

You've found a creative way to build some initial visibility into that potential value.

You've taken the time to practice tactical empathy, and then deployed your newly practiced skills on building rapport with your key stakeholder.

It's now the Big Day.

You're ready to make your case for investment.

You speak to both your stakeholder's fears and their hopes.

You situate the investment in line with their long-term goals.

You offer the initial visibility that you've built on the side as a means to understand why investment is indicated now and not later.

To your surprise and delight... your stakeholder is convinced.

# They're ready to make the difficult decision to temporarily put aside some of their other priorities.

# Temporarily.

Then, they ask: "So, how long will this take?"

And stare expectantly at you.

And, boom, you're right back in one of the most fun parts of being an engineering leader.


** Old Intro To "Estimates Are Hard"
Obviously, estimates are always tricky.

But they're often *extra* tricky for technical investments.

When digging into complex and uncertain parts of your systems (of both the technical and social varieties), it can be difficult when you start to even *understand your options*.

Which makes it nearly impossible to commit to a time estimate to achieve some cleanly visible outcome.

# Satoe Sakuma said this well, "It's like you're going into a cave, and the first step is just bringing in some light. But then, when they ask you how long it will take"

Once again, let's put ourselves in our stakeholder's shoes.

Let's say we have a stakeholder who is not yet in a habit of regularly co-prioiritizing technical investments with engineering.

Even if that stakeholder fully embraces an iterative approach to software development (which is a best case, not always achieved in reality)...

...there's still a very good chance that, as they're considering a technical investment you've proposed, they're trying to limit the impact of this "unpleasant temporary distraction" on their current goals.

They're reviewing the sprawling list of things things they've been asked to do.

They're reviewing the painfully smaller list of things they believe are possible -- and getting ready to eject a few things from that list.

# They're actively rejiggering their mental map of the next month or two.

They're thinking, "Which people am I going to disappoint in order to solve this 'engineering problem'?"[fn:: It's not an engineering problem! It's an opportunity for investment. You'll get there eventually]

They're likely preparing to bargain you down from whatever your initial "request" for time turns out to be.[fn:: You're not making a request! You're exposing an opportunity for investment. Again, you'll get there eventually]

# Remember their job is to disappoint everyone around them.

So you have to say *something* to them -- you can't just say "I don't know, it'll take as long as it takes".



** Old Scenario 1: Unreliable Deploys

The deploy pipeline randomly fails for no clear reason, and when the team does successfully deploy, they end up reverting far too often.

You have some developed some initial visibility showing that engineers are starting to deploy less frequently as a result.

That, plus a some heavy leaning on Accelerate has your stakeholder ready to talk.

They say, "How long will this take to fix?"

How can you respond?

If you knew *why* the pipeline was failing... it wouldn't be failing.

Ditto for the post-deploy failures and reverts.

You could take a *guess*, based on the team's current hunches about what's causing the unreliability, and say something like "We think we can likely improve things in about three weeks of time, for two engineers."

What could those two engineers find when they dig in?

If you're lucky, maybe they find some misconfigured CI/CD params. You're just a couple of arcane JSON or YAML config tweaks away from more reliable deploys.

If you're *not* so lucky, they might discover that your entire build pipeline is, for deep-in-the-guts reasons, non-deterministically selecting versions to build, test and deploy. To get to a more reliable process, you're going to have to restructure that pipeline at a fairly deep level.

If you're *super, super not lucky*, this might be the moment you realize that fundamental architectural choices made long ago are forcing multiple teams to all frequently change the same hotly contested parts of your codebase, and that's causing *genuine conflicts*. The repeated deploy failures are actually the CI/CD process *doing its job*. You don't have to update your deploy process... you just have to update the most fundamental abstractions in your code. Easy peasy.

In the latter two cases, you're going to reach the end of your three week period, and not only is your stakeholder going to expect everyone to return to fully focusing on the original feature work, but, *you won't be able to show them any visible improvements*.

Instead of this building trust and making your next investment easier to advocate for, you'll feel like you've dug yourself into a hole.
** Tech Investments Aren't Exactly Projects and Don't Exactly End

It's extremely tempting to think about tech investments through the lens of "projects".

We've spec'd out a project, and we're going to do X, Y and Z, and be done by such-and-such a date.

There's something useful in this, but you have to be very careful about how you define the "goal" of the project.

To see why, let's return to our friend Bertha the economically rational investor.

Say you've convinced a stakeholder to "let" your team work on the problem of sluggish site performance. That stakeholder has asked you how long the work will take.

Now, you want to create as much value for your business as possible.

So you ask Bertha, the arbiter of value:

/Given what you know, how long *should* my team work on improving site performance? When should we tell our stakeholder that the work will be over? What will maximize company value?/

Bertha would squint at you, and then say:

/If you want to maximize value for the company, your team should work on improving site performance until precisely the moment that the net benefit of further site performance improvements is less than the net benefit of other work you could be doing, instead./

Say you've just finished a project to deal with sluggish site performance.

You SLI's have achieved an acceptable range, given current load.

Everyone celebrates the "Improve Performance" project being over. Maybe there's a party.

But then the next day, your site gets a huge influx of new users.

These new users adore the base feature set, have very high expectations of responsiveness, and could care less about all the clever new extensions product has dreamed up.

Given the above, it's likely economically optimal for your team to keep working on site performance, and not pivot back to expanding the feature set.

But if you let yourself get drawn too tightly into the project formulation, no one will even be thinking about further potential site performance investments

In fact, if you're extra unlucky, stakeholders who "gave you time" to "fix" the performance are going to be mad, because the site suddenly seems really sluggish again (see: huge increase in usage). What, are they supposed to let the team spend another month on these engineering issues?!? When are they going to get back to business needs?!

Instead of thinking about tech investments primarily as projects, I think you're better served by thinking of them as a series of *decision points*.

You do some chunk of work, make some improvements, build slightly clearer visibility. Then, based on what you've discovered, you decide: should we keep going down this road?

# The companies that make better decisions, more often, are the companies that win.

You want to get your stakeholder into a regular cadence of shared decision-making.

# As part of that, you may need to retrain yourself to think about tech investments not as one-off projects, but as a steady series of opportunities to make choices together.

# This can be particularly hard if you engineering team has developed a scarcity mindset around engineering-driven work. E.g. if you're only ever able to bargain your way into three weeks of "engineering" work in any year, it can feel like the idea of planning for a future shared decision is a fool's move.

/"Fine, Dan, whatever, that *sounds* great"/ I can imagine you thinking, /"but how on earth am I supposed to get my stakeholder to buy into that?"/

Your stakeholder is standing in front of you.

They're waiting for you to tell them how long your first proposed investment is going to take.

They are clearly *not* looking forward to "repeated discussions about potential tech investments", in the future.

What do you say to them?

If you've read this far, you'll know that I've helpfully tested out the strategy of "Explain the abstract theory of the value of decisions" to them, and seen it fail 100% of the time, and am ready to instead share what I *have* seen work.
** Marketing
Imagine we flip this around, to some part of the business where leaders are used to thinking about "positive" investments.

For most B2B businesses, few things are more important than acquiring new, high-quality leads -- potential customers their sales team can talk to.

# If you're helping run a B2B business, odds are good that you care very much about bringing high quality leads into the top of your sales funnel.

Most B2B businesses therefore have a marketing team.

That team spends various amounts of money, to run various campaigns, across various platforms, to acquire leads.

They know how many dollars they spend, on average, to acquire a high-quality lead.

They've developed a plan, which they're currently executing, spending money every month against various platforms.

But then, one day, a new platform shows up, that the company has never advertised on before.

The marketing team runs an initial campaign, spending $1,000.

They promptly get back more high quality leads per dollar spent than on any other platform!

Amazing.

Should they now stop? Because that initial project is "over"? And they have a "plan", they need to get back to?

Of course not.

They should *change the plan*, based on the new information.

If, by making an initial investment, they've discovered that there is even more value to be found, they should *increase* their investment -- not curtail it.

Many tech investments function just this way -- there's a potential source of value, once you dig into it, you may very well find *more* value than you realized, and more than you were expecting to make, from your existing roadmap.

Sometimes that's visibly positive value: "It'll only take another week to apply the new indexing scheme to the rest of our tables, and then the entire site should see better peformance".

Sometimes that's risk-avoidance value: "Our initial security review made clear that things are much worse than we realized -- we think it's more valuable for the company to pause work on the feature roadmap and get to a lower risk state, immediately."

But if your "engineering project" is "over", no one is going to be looking for further investment opportunities.

Because, fundamentally, they're thinking about it is an "unpleasant" project they've been forced to do, not an investment that makes their company more valuable.

You're going to gradually persuade people into working that way.
